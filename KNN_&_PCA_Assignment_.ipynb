{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**KNN & PCA  Assignment**"
      ],
      "metadata": {
        "id": "0nykTK1QTHCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANS 1-** K-Nearest Neighbors (KNN) is a supervised, instance-based learning algorithm. It does not learn a model during training; instead, it stores the entire dataset and makes predictions based on similarity.\n",
        "\n",
        "In classification, KNN looks at the K nearest data points to a new observation and assigns the class that appears most frequently among them.\n",
        "\n",
        "In regression, it predicts the average (or sometimes weighted average) of the target values of the K nearest neighbors.\n",
        "\n",
        "The key idea behind KNN is that similar data points tend to have similar outputs."
      ],
      "metadata": {
        "id": "UGAnlBszTaAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANS2 -** The Curse of Dimensionality refers to problems that arise when the number of features becomes very large.\n",
        "\n",
        "As dimensions increase:\n",
        "\n",
        "- Distances between data points become less meaningful\n",
        "\n",
        "- All points start appearing equally far away\n",
        "\n",
        "- KNN struggles to identify true “nearest” neighbors\n",
        "\n",
        "Since KNN relies entirely on distance calculations, high-dimensional data significantly reduces its accuracy and increases computational cost."
      ],
      "metadata": {
        "id": "PbkFhiWcThD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANS3-** Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms original features into a smaller set of new features called principal components.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "PCA creates new features by combining existing ones\n",
        "\n",
        "Feature selection keeps a subset of original features and discards others\n",
        "\n",
        "PCA focuses on maximizing variance, while feature selection focuses on choosing relevant variables."
      ],
      "metadata": {
        "id": "BlaDG3hcTrY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANS4 -**\n",
        "- Eigenvectors represent the directions (principal components) along which the data varies the most.\n",
        "\n",
        "- Eigenvalues represent how much variance is captured along each eigenvector.\n",
        "\n",
        "They are important because PCA ranks components using eigenvalues and keeps the ones that explain the most variance, helping reduce dimensionality while preserving information."
      ],
      "metadata": {
        "id": "WpO7cA1eTzWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS5- KNN suffers in high-dimensional spaces, while PCA reduces dimensionality.\n",
        "\n",
        "By applying PCA before KNN:\n",
        "\n",
        "- Noise and redundant features are reduced\n",
        "\n",
        "- Distance calculations become more meaningful\n",
        "\n",
        "- Model accuracy and efficiency improve\n",
        "\n",
        "This combination is especially effective for datasets with many correlated features."
      ],
      "metadata": {
        "id": "iugteEwaUIRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS6 - KNN with and without Feature Scaling (Wine Dataset)\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RQ1ON5QUTiX",
        "outputId": "a8d51634-4c6e-49d8-a474-d11a49d5ccd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS7- PCA Explained Variance Ratio\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, _ = load_wine(return_X_y=True)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csOeuW2sUxNt",
        "outputId": "114355e5-c64f-4323-cdb3-1e4e6dfaf9ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS8-KNN on PCA-Reduced Data (Top 2 Components)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with PCA (2 components):\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TUc1ml1U4gB",
        "outputId": "9f34704f-10fa-438e-e9da-559041b25bb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS9- KNN with Different Distance Metrics\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Euclidean accuracy:\",\n",
        "      accuracy_score(y_test, knn_euclidean.predict(X_test_scaled)))\n",
        "\n",
        "print(\"Manhattan accuracy:\",\n",
        "      accuracy_score(y_test, knn_manhattan.predict(X_test_scaled)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8JKeuYCVNxl",
        "outputId": "8556898c-fb51-4054-a744-8afc90c59b35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean accuracy: 0.9629629629629629\n",
            "Manhattan accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS10- Gene expression datasets have thousands of features but very few samples, making overfitting a major concern.\n",
        "\n",
        "- **Step 1: PCA**\n",
        "\n",
        "- Standardize data\n",
        "\n",
        "- Apply PCA to remove noise and correlated genes\n",
        "\n",
        "- Retain components explaining ~90–95% variance\n",
        "\n",
        "- **Step 2: Choosing Components**\n",
        "\n",
        "- Use cumulative explained variance plot\n",
        "\n",
        "- Balance information retention and simplicity\n",
        "\n",
        "- **Step 3: KNN Classification**\n",
        "\n",
        "- Train KNN on PCA-transformed data\n",
        "\n",
        "- Reduced dimensionality improves neighbor reliability\n",
        "\n",
        "- **Step 4: Evaluation**\n",
        "\n",
        "- Cross-validation\n",
        "\n",
        "- Accuracy, precision, recall\n",
        "\n",
        "- Confusion matrix\n",
        "\n",
        "- **Business Justification**\n",
        "\n",
        "- Reduces overfitting\n",
        "\n",
        "- Improves generalization\n",
        "\n",
        "- Makes model more stable and interpretable for biomedical use"
      ],
      "metadata": {
        "id": "F3ckGw7xVZ9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n"
      ],
      "metadata": {
        "id": "v9B8NMY0W9ml"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}